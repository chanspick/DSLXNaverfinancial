{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100","mount_file_id":"1CuEB3su4WP0g1rk8WfsIk57-2VuxYfbF","authorship_tag":"ABX9TyMRHGFSvevVFuUgnP4dJ3Zf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!pip install keybert"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OcU-j0Ipq4dN","executionInfo":{"status":"ok","timestamp":1684288752163,"user_tz":-540,"elapsed":20933,"user":{"displayName":"ee tr","userId":"00238655616217514787"}},"outputId":"e3d1415a-1d5b-4e4a-807d-bd3ad69ad4d3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting keybert\n","  Downloading keybert-0.7.0.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting sentence-transformers>=0.3.8 (from keybert)\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.2.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.22.4)\n","Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from keybert) (13.3.4)\n","Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (2.2.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (2.14.0)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (3.1.0)\n","Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers>=0.3.8->keybert)\n","  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.65.0)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (2.0.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.15.1+cu118)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (3.8.1)\n","Collecting sentencepiece (from sentence-transformers>=0.3.8->keybert)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence-transformers>=0.3.8->keybert)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.12.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2023.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.27.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (4.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (23.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (16.0.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2022.10.31)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=0.3.8->keybert) (8.1.3)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (8.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (2.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n","Building wheels for collected packages: keybert, sentence-transformers\n","  Building wheel for keybert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keybert: filename=keybert-0.7.0-py3-none-any.whl size=23777 sha256=8e2f9e2ee0bc1e43667b3cb085a98dd1bd2d7cad5358f29a83be08724c4ed5e9\n","  Stored in directory: /root/.cache/pip/wheels/66/8d/e6/b0e2f8d883b0fd51819226f67ad9843e04913ce4a97241ff4b\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=a77a836dfa27fd3326198bab14a1803856592df051b55cea92a8b89d6706d458\n","  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n","Successfully built keybert sentence-transformers\n","Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence-transformers, keybert\n","Successfully installed huggingface-hub-0.14.1 keybert-0.7.0 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.29.2\n"]}]},{"cell_type":"code","execution_count":70,"metadata":{"id":"jGcslm6nqfM0","executionInfo":{"status":"ok","timestamp":1684299507686,"user_tz":-540,"elapsed":470,"user":{"displayName":"ee tr","userId":"00238655616217514787"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import random\n","import re\n","import requests\n","from keybert import KeyBERT\n","from scipy import spatial \n","from sentence_transformers import SentenceTransformer, models\n","\n","# Make Sample list(Ko, En)\n","\n","def Make_En():\n","    Table = pd.read_csv('/content/drive/MyDrive/task1/task1/Sample.csv')\n","    Table = Table.iloc[:, 1:]\n","    Table.dropna(axis = 0, inplace = True)\n","    Table_list = list(Table.comment)\n","\n","    Table_list_en = []\n","    for elem_ko in Table_list:\n","        elem_en = get_translate(elem_ko)\n","        Table_list_en.append(elem_en)\n","\n","    return Table, Table_list, Table_list_en\n","\n","# Translate Ko -> En\n","\n","def get_translate(text):\n","    client_id = '9aorPMFuECuHYdvjMjYV'\n","    client_secret = 'DYCcQFJTt7'\n","    data = {'text' : text,\n","            'source' : 'ko',\n","            'target' : 'en'}\n","    url = \"https://openapi.naver.com/v1/papago/n2mt\"\n","    header = {'X-Naver-Client-Id' : client_id,\n","              'X-Naver-Client-Secret' : client_secret}\n","    response = requests.post(url, headers = header, data = data)\n","    rescode = response.status_code\n","    if (rescode == 200):\n","        send_data = response.json()\n","        trans_data = (send_data['message']['result']['translatedText']) # return value\n","        return trans_data\n","    else:\n","        print('Error Code:', rescode)\n","\n","# Make Keyword\n","\n","def KeyWordExtracting_Single(doc_en):\n","     kw_model = KeyBERT()\n","     keywords_1 = kw_model.extract_keywords(doc_en, keyphrase_ngram_range=(2, 4) , use_mmr=True, diversity=0.60, top_n = 5)\n","     keywords_2 = kw_model.extract_keywords(doc_en, keyphrase_ngram_range=(2, 4) , use_maxsum=True, nr_candidates=20, top_n = 3)\n","\n","     return keywords_1, keywords_2\n","\n","def KeyWordExtracting_Multiple(doc_en):\n","    kw_model = KeyBERT()\n","    doc_modified = doc_en.replace(' and', ',').replace(' or', ',')\n","    doc_list = doc_modified.split(',')\n","    KEYWORDS = list()\n","    for index, doc in enumerate(doc_list):\n","        if len(doc.split()) <= 2:\n","            continue\n","        else:\n","            keywords_1 = kw_model.extract_keywords(doc, keyphrase_ngram_range=(2, 4) , use_mmr=True, diversity=0.75, top_n = 5)\n","            keywords_2 = kw_model.extract_keywords(doc, keyphrase_ngram_range=(2, 4) , use_maxsum=True, nr_candidates=20, top_n = 3)\n","            KEYWORDS.append((keywords_1, keywords_2))\n","    return KEYWORDS\n","\n","def best_cos_sim(table_list: list, keyword_list_1: list, keyword_list_2: list):\n","    st_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n","    best_list = []\n","\n","    # column = 테이블 컬럼명, keyword_list = 추출된 키워드 리스트\n","    # 추출된 키워드 element 별 해당 컬럼명과 코사인 유사도 비교 & 평균\n","    # 키워드 리스트 element 들과 컬럼의 유사도 평균값들을 리스트에 저장\n","    # 저장된 리스트에서 가장 높은 평균 코사인 유사도를 가진 영문 번역 컬럼 인덱스 가져옴\n","    # return 값으로 제일 높은 평균 코사인 유사도를 가진 영문 번역 컬럼 반환\n","\n","    for column in table_list:\n","        column_embedding = st_model.encode(column).flatten()\n","        sim_total = 0\n","        for keyword in keyword_list_1:\n","            keyword = str(keyword)\n","            keyword = keyword.split(',')[0].strip('(')\n","            keyword_embedding = st_model.encode(keyword).flatten()\n","            sim = 1 - spatial.distance.cosine(column_embedding, keyword_embedding) # compute cosine similarity between two vectors\n","            sim_total += sim\n","        for keyword in keyword_list_2:\n","            keyword = str(keyword)\n","            keyword = keyword.split(',')[0].strip('(')\n","            keyword_embedding = st_model.encode(keyword).flatten()\n","            sim = 1 - spatial.distance.cosine(column_embedding, keyword_embedding) # compute cosine similarity between two vectors\n","            sim_total += sim\n","        sim_mean = sim_total / len(keyword_list_1 + keyword_list_2)\n","        best_list.append(sim_mean)\n","    best_index = best_list.index(max(best_list))\n","    return table_list[best_index]\n","\n","def Make_Q_En(Table, doc_en, best_en, Table_list_en, Table_list):\n","    ko_index = Table_list_en.index(best_en)\n","    best_ko = Table_list[ko_index]\n","    print(f'best translation : {best_en} \\nKorean: {best_ko}')\n","    best_row_Table = Table[Table['comment'] == best_ko]\n","    if len(best_row_Table) != 1:\n","        Table_name = list(best_row_Table['table_name'])[0]\n","        col_name = list(best_row_Table['col_name'])[0]\n","        type_name = list(best_row_Table['data_type'])[0]\n","        prompt = doc_en +'\\n'+f'Table_name :{Table_name}, Column_name : {col_name}, Type : {type_name}'\n","    else:\n","        Table_name = list(best_row_Table['table_name'])[0]\n","        col_name = list(best_row_Table['col_name'])[0]\n","        type_name = list(best_row_Table['data_type'])[0]\n","        prompt = doc_en +'\\n'+f'Table_name :{Table_name}, Column_name : {col_name}, Type : {type_name}'\n","    return prompt\n","\n","def Make_Q_En_Multiple(Table, doc_en, best_en_list: list, Table_list_en, Table_list):\n","    best_prompt = []\n","    prompt = doc_en\n","    for best_en in best_en_list:\n","        ko_index = Table_list_en.index(best_en)\n","        best_ko = Table_list[ko_index]\n","        print(f'best translation : {best_en} \\nKorean: {best_ko}')\n","        best_row_Table = Table[Table['comment'] == best_ko]\n","\n","        if len(best_row_Table) != 1:\n","            Table_name = list(best_row_Table['table_name'])[0]\n","            col_name = list(best_row_Table['col_name'])[0]\n","            type_name = list(best_row_Table['data_type'])[0]\n","            best_prompt.append((best_en, Table_name, col_name, type_name))\n","        else:\n","            Table_name = list(best_row_Table['table_name'])[0]\n","            col_name = list(best_row_Table['col_name'])[0]\n","            type_name = list(best_row_Table['data_type'])[0]\n","            best_prompt.append((best_en, Table_name, col_name, type_name))\n","    for bundle in best_prompt:\n","        best_en, Table_name, col_name, type_name = bundle\n","        prompt += '\\n'+f'Table_name of {best_en}:{Table_name}, Column_name of {best_en} : {col_name}, Type of {col_name} : {type_name}'\n","    return prompt\n","\n","\n","\n","\n","def prompt_generation(doc): #prompt generation\n","    Table, Table_list, Table_list_en = Make_En()\n","    \n","    doc_en = get_translate(doc)\n","    #identify single, multiple\n","    if ('and' in doc_en) or ('or' in doc_en):\n","        keywords_list = KeyWordExtracting_Multiple(doc_en)\n","        print(keywords_list)\n","\n","        best_en_list = []\n","\n","        for keywords in keywords_list:\n","            keywords_1, keywords_2 = keywords\n","            best_en = best_cos_sim(Table_list_en, keywords_1, keywords_2)\n","            best_en_list.append(best_en)\n","        prompt = Make_Q_En_Multiple(Table, doc_en, best_en_list, Table_list_en, Table_list)\n","        return prompt\n","                    \n","\n","    else:\n","        keywords_1, keywords_2 = KeyWordExtracting_Single(doc_en)\n","        best_en = best_cos_sim(Table_list_en, keywords_1, keywords_2)\n","        prompt = Make_Q_En(best_en, Table_list_en, Table_list)\n","        print(prompt)\n","        return prompt\n"]},{"cell_type":"code","source":["def prompt_generation(doc):\n","    Table, Table_list, Table_list_en = Make_En()\n","    \n","    doc_en = get_translate(doc)\n","    #identify single, multiple\n","    if ('and' in doc_en) or ('or' in doc_en):\n","        keywords_list = KeyWordExtracting_Multiple(doc_en)\n","        print(keywords_list)\n","\n","        best_en_list = []\n","\n","        for keywords in keywords_list:\n","            keywords_1, keywords_2 = keywords\n","            best_en = best_cos_sim(Table_list_en, keywords_1, keywords_2)\n","            best_en_list.append(best_en)\n","        prompt = Make_Q_En_Multiple(Table, doc_en, best_en_list, Table_list_en, Table_list)\n","        return prompt\n","                    \n","\n","    else:\n","        keywords_1, keywords_2 = KeyWordExtracting_Single(doc_en)\n","        best_en = best_cos_sim(Table_list_en, keywords_1, keywords_2)\n","        prompt = Make_Q_En(best_en, Table_list_en, Table_list)\n","        print(prompt)\n","        return prompt"],"metadata":{"id":"SXTz8NqIa7ac","executionInfo":{"status":"ok","timestamp":1684298642882,"user_tz":-540,"elapsed":1165,"user":{"displayName":"ee tr","userId":"00238655616217514787"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SOJFk_79GE2e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["doc = input()\n","doc_en = get_translate(doc)\n","if ('and' in doc_en) or ('or' in doc_en):\n","    keywords_list = KeyWordExtracting_Multiple(doc_en)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UNMlJW1-bBnj","executionInfo":{"status":"ok","timestamp":1684299497644,"user_tz":-540,"elapsed":4441,"user":{"displayName":"ee tr","userId":"00238655616217514787"}},"outputId":"3587784f-770a-4376-fdef-ff1d41b84da7"},"execution_count":68,"outputs":[{"name":"stdout","output_type":"stream","text":["네이버페이 회원번호가 0으로 시작하고 은행명이 국민은행인 행들을 추출해 줘\n"]}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xktKWLZZex3l","executionInfo":{"status":"ok","timestamp":1684299497646,"user_tz":-540,"elapsed":21,"user":{"displayName":"ee tr","userId":"00238655616217514787"}},"outputId":"52b27ba0-6ecd-4630-fd1c-1ab29c62070b"},"execution_count":69,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[([('naver pay membership number', 0.8702),\n","   ('pay membership number starts', 0.7353),\n","   ('membership number', 0.5486),\n","   ('pay membership', 0.5221),\n","   ('number starts', 0.433)],\n","  [('naver pay', 0.6179), ('membership number starts', 0.6299)]),\n"," ([('bank kookmin bank', 0.9279),\n","   ('kookmin bank', 0.9076),\n","   ('bank kookmin', 0.8887)],\n","  [('kookmin bank', 0.9076), ('bank kookmin bank', 0.9279)])]"]},"metadata":{},"execution_count":69}]},{"cell_type":"code","source":["doc_en"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"4D-JNC9TDIkW","executionInfo":{"status":"ok","timestamp":1684299474428,"user_tz":-540,"elapsed":728,"user":{"displayName":"ee tr","userId":"00238655616217514787"}},"outputId":"9a1ea720-0744-476d-d881-ec4f210e43d9"},"execution_count":67,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Naver Pay membership number starts with 0 and the bank name is Kookmin Bank'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["best_list = []\n","for keywords in keywords_list:\n","    keywords_1, keywords_2 = keywords\n","    best_en = best_cos_sim(keywords_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"ts8Jo0eTvpuU","executionInfo":{"status":"ok","timestamp":1684298979085,"user_tz":-540,"elapsed":509,"user":{"displayName":"ee tr","userId":"00238655616217514787"}},"outputId":"0b6ca0c9-b8b9-4926-ad0e-4321619c1862"},"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Naver Pay membership number starts with 0, the bank name is Kookmin Bank'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["B = 'Extract the lines where the membership number of Navi Pay starts with 0 and the payment product name starts with A'\n","if 'and' in B or 'or' in B:\n","    print('X')\n","else:\n","    print('O')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LNabCBn2vyAN","executionInfo":{"status":"ok","timestamp":1684298277025,"user_tz":-540,"elapsed":583,"user":{"displayName":"ee tr","userId":"00238655616217514787"}},"outputId":"60512544-23e0-4691-bd6f-3661b606ad64"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["X\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"JBqcsORy_lHD"},"execution_count":null,"outputs":[]}]}